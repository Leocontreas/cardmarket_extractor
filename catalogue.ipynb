import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import pandas as pd
import time

# Set up the Edge web driver
driver = webdriver.Edge()

# Login URL
driver.get('https://www.cardmarket.com/en/Login')

# Enter login credentials
driver.find_element(By.NAME, 'username').send_keys('your_email')
driver.find_element(By.NAME, 'userPassword').send_keys('your_password')
driver.find_element(By.NAME, 'userPassword').send_keys(Keys.RETURN)

# Function to extract data from a page
def extract_data(soup):
    card_elements = soup.find_all('div', class_='article-row')
    data = []

    for card in card_elements:
        item_name_full = card.find('div', class_='col-seller').find('a').text.strip()
        
        # Split the item name to differentiate between card title and item name
        if '(' in item_name_full:
            card_title, item_name = item_name_full.split(' (', 1)
            item_name = f'{card_title} ({item_name}'  # Re-add the opening parenthesis for the full item name
        else:
            card_title = item_name_full
            item_name = item_name_full
        
        expansion = card.find('a', class_='expansion-symbol').get('data-bs-original-title', 'N/A')
        rarity = card.find('svg').get('data-bs-original-title', 'N/A')
        condition = card.find('a', class_='article-condition').get('data-bs-original-title', 'N/A')
        language_span = card.find('span', class_='icon me-2')
        language = language_span.get('data-original-title', 'N/A')
        quantity = card.find('div', class_='mobile-offer-container').find('span', class_='item-count').text.strip()
        price = card.find('div', class_='mobile-offer-container').find('span', class_='color-primary').text.strip()
        
        # Extract extra details (e.g., First Edition, Reverse Holo)
        extra_details = []
        special_icons = card.find_all('span', class_='icon')
        for icon in special_icons:
            extra_detail = icon.get('data-original-title', '')
            if extra_detail and extra_detail != language:  # Avoid duplicating language info
                extra_details.append(extra_detail)
        
        extra_details = ', '.join(extra_details)

        # Extract extra notes (e.g., holo, galaxy holo, damaged)
        product_comments = card.find('div', class_='product-comments')
        extra_notes = ''
        if product_comments:
            text_notes = product_comments.find('span', class_='d-block text-truncate text-muted fst-italic small')
            icon_notes = product_comments.find('span', class_='fonticon-comments')
            if text_notes and text_notes.text.strip():
                extra_notes = text_notes.text.strip()
            elif icon_notes and icon_notes.get('data-bs-original-title', '').strip():
                extra_notes = icon_notes.get('data-bs-original-title', '').strip()
        
        if not extra_notes:
            extra_notes = ' '

        data.append([card_title, item_name, expansion, rarity, condition, language, quantity, price, extra_details, extra_notes])

    return data

# Initialize the DataFrame
df = pd.DataFrame(columns=['Card Title', 'Item Name', 'Expansion', 'Rarity', 'Condition', 'Language', 'Quantity', 'Price', 'Extra Details', 'Extra Notes'])

# Base URL for the personal page
base_url = 'https://www.cardmarket.com/en/Pokemon/Stock/Offers/Singles?site='
page_number = 13

while True:
    # Construct the URL for the current page
    page_url = base_url + str(page_number)
    print(f'Processing page URL: {page_url}')  # Debugging: print the URL
    driver.get(page_url)
    
    # Extract data from the current page
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')
    data = extract_data(soup)
    
    # Debugging: print the data extracted from the current page
    print(f'Data from page {page_number}: {data}')
    
    # Check if data is extracted; if not, break the loop
    if not data:
        break
    
    # Add data to DataFrame
    temp_df = pd.DataFrame(data, columns=['Card Title', 'Item Name', 'Expansion', 'Rarity', 'Condition', 'Language', 'Quantity', 'Price', 'Extra Details', 'Extra Notes'])
    df = pd.concat([df, temp_df], ignore_index=True)
    
    # Increment page number
    page_number += 1
    time.sleep(2)  # Wait for the next page to load

# Remove duplicate rows
df = df.drop_duplicates()

# Save to CSV
df.to_csv('catalogue.csv', index=False)

print('Data extraction completed successfully and duplicates removed.')
