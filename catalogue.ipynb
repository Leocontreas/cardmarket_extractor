import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import pandas as pd
import time

email = 'insert_your_email'
password = 'insert_your_password'

# Set up the Edge web driver
driver = webdriver.Edge()

# Login URL
driver.get('https://www.cardmarket.com/en/Login')

# Enter login credentials
driver.find_element(By.NAME, 'username').send_keys(email)
driver.find_element(By.NAME, 'userPassword').send_keys(password)
driver.find_element(By.NAME, 'userPassword').send_keys(Keys.RETURN)

# Initialize the DataFrame
final_df = pd.DataFrame(columns=['Card Title', 'Item Name', 'Expansion', 'Rarity', 'Condition', 'Language', 'Quantity', 'Price', 'Extra Details', 'Extra Notes'])
base_url = 'https://www.cardmarket.com/en/Pokemon/Stock/Offers/Singles?'
driver.get(page_url)

# Function to extract data from a page
def extract_data(soup):
    card_elements = soup.find_all('div', class_='article-row')
    data = []

    for card in card_elements:
        item_name_full = card.find('div', class_='col-seller').find('a').text.strip()
        
        # Split the item name to differentiate between card title and item name
        if '(' in item_name_full:
            card_title, item_name = item_name_full.split(' (', 1)
            item_name = f'{card_title} ({item_name}'  # Re-add the opening parenthesis for the full item name
        else:
            card_title = item_name_full
            item_name = item_name_full
        
        expansion = card.find('a', class_='expansion-symbol').get('data-bs-original-title', 'N/A')
        rarity = card.find('svg').get('data-bs-original-title', 'N/A')
        condition = card.find('a', class_='article-condition').get('data-bs-original-title', 'N/A')
        language_span = card.find('span', class_='icon me-2')
        language = language_span.get('data-original-title', 'N/A')
        quantity = int(card.find('div', class_='mobile-offer-container').find('span', class_='item-count').text.strip())
        price = float(card.find('div', class_='mobile-offer-container').find('span', class_='color-primary').text.strip()[1:])
        
        # Extract extra details (e.g., First Edition, Reverse Holo)
        extra_details = []
        special_icons = card.find_all('span', class_='icon')
        for icon in special_icons:
            extra_detail = icon.get('data-original-title', '')
            if extra_detail and extra_detail != language:  # Avoid duplicating language info
                extra_details.append(extra_detail)
        
        extra_details = ', '.join(extra_details)

        # Extract extra notes (e.g., holo, galaxy holo, damaged)
        product_comments = card.find('div', class_='product-comments')
        extra_notes = ''
        if product_comments:
            text_notes = product_comments.find('span', class_='d-block text-truncate text-muted fst-italic small')
            icon_notes = product_comments.find('span', class_='fonticon-comments')
            if text_notes and text_notes.text.strip():
                extra_notes = text_notes.text.strip()
            elif icon_notes and icon_notes.get('data-bs-original-title', '').strip():
                extra_notes = icon_notes.get('data-bs-original-title', '').strip()
        
        if not extra_notes:
            extra_notes = ' '

        data.append([card_title, item_name, expansion, rarity, condition, language, quantity, price, extra_details, extra_notes])

    return data

# Function to process each expansion set
def process_expansion_set(expansion_id):
    # Construct the base URL for the current expansion set
    base_url = f'https://www.cardmarket.com/en/Pokemon/Stock/Offers/Singles?idExpansion={expansion_id}&site='
    
    # Initialize DataFrame for the current expansion
    expansion_df = pd.DataFrame(columns=['Card Title', 'Item Name', 'Expansion', 'Rarity', 'Condition', 'Language', 'Quantity', 'Price', 'Extra Details', 'Extra Notes'])

    page_number = 1
    while True:
        # Construct the URL for the current page
        page_url = base_url + str(page_number)
        driver.get(page_url)
        time.sleep(2)  # Wait for the page to load
        
        # Extract data from the current page
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        data = extract_data(soup)
        
        # Check if data is extracted; if not, break the loop
        if not data:
            break
        
        # Add data to DataFrame
        temp_df = pd.DataFrame(data, columns=['Card Title', 'Item Name', 'Expansion', 'Rarity', 'Condition', 'Language', 'Quantity', 'Price', 'Extra Details', 'Extra Notes'])
        expansion_df = pd.concat([expansion_df, temp_df], ignore_index=True)
        
        # Increment page number
        page_number += 1
        time.sleep(2)  # Wait for the next page to load

    return expansion_df

# Iterate through each expansion set and collect data
select_element = driver.find_element(By.NAME, 'idExpansion')
options = select_element.find_elements(By.TAG_NAME, 'option')
for option in range(1, len(options)):  # Skip the first option (All)
    expansion_id = options[option].get_attribute('value')
    expansion_df = process_expansion_set(expansion_id)
    final_df = pd.concat([final_df, expansion_df], ignore_index=True)
    
    # Refresh references to the select element and options to avoid stale element exception
    select_element = driver.find_element(By.NAME, 'idExpansion')
    options = select_element.find_elements(By.TAG_NAME, 'option')

# Remove duplicate rows
#final_df = final_df.drop_duplicates()

final_df.columns = ['Card_Title','Item_Name', 'Expansion', 'Rarity', 'Condition', 'Language', 'Quantity', 'Price', 'Extra Details', 'Comments']
final_df = final_df[['Card_Title','Item_Name', 'Expansion', 'Rarity', 'Condition', 'Language', 'Extra Details', 'Comments', 'Quantity', 'Price']]
# Save to CSV
final_df.to_csv('catalogue.csv', index=False)

print('Data extraction completed successfully.')
